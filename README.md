# ğŸ•·ï¸ Large Scraper for Senior Living Facilities

This project is a robust, modular web scraping pipeline built with [Playwright](https://playwright.dev/) (both sync and async), designed to collect and enrich data on over 200,000 senior living facilities from [aplaceformom.com](https://www.aplaceformom.com/).

---

## ğŸ“ Project Structure

```
Large-scraper/
â”œâ”€â”€ scrapers/
â”‚   â”œâ”€â”€ 1_parse_sitemap.py
â”‚   â”œâ”€â”€ 2_extract_facility_links.py
â”‚   â”œâ”€â”€ 3_scrape_facility_details.py
â”‚   â””â”€â”€ 4_enrich_with_google.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ facility_urls.csv
â”‚   â”œâ”€â”€ community_links.csv
â”‚   â”œâ”€â”€ facility_data.xlsx
â”‚   â”œâ”€â”€ facility_data_enriched.xlsx
â”‚   â””â”€â”€ failed_urls.csv
```

---

## ğŸ§  How It Works

### 1ï¸âƒ£ Parse Sitemap
`scrapers/1_parse_sitemap.py`  
- Downloads and parses the sitemap XML from the source website.
- Extracts URLs pointing to location-specific pages.
- Saves results to: `data/facility_urls.csv`

### 2ï¸âƒ£ Extract Facility Links
`scrapers/2_extract_facility_links.py`  
- Opens each location URL from `facility_urls.csv`.
- Extracts individual facility page links.
- Saves results to: `data/community_links.csv`

### 3ï¸âƒ£ Scrape Facility Details
`scrapers/3_scrape_facility_details.py`  
- Visits each facility URL from `community_links.csv`.
- Extracts:
  - Facility name
  - Address
  - Type (e.g., Assisted Living, Independent Living)
- Saves results to: `data/facility_data.xlsx`

### 4ï¸âƒ£ Enrich with Google Search
`scrapers/4_enrich_with_google.py`  
- Uses Google search to find:
  - Phone number
  - Official website URL
- Takes names from `facility_data.xlsx`, enriches records.
- Saves enriched data to: `data/facility_data_enriched.xlsx`

---

## âš™ï¸ Features

âœ… Built for **large-scale scraping** (200k+ URLs)  
âœ… **User-Agent rotation** to avoid bans  
âœ… **Batching, throttling, delays, and semaphores** for controlled concurrency  
âœ… **Retry mechanism** via `data/failed_urls.csv`  
âœ… Organized modular design (each step is restartable)  
âœ… Playwright-powered with support for both **sync** and **async** flows  
âœ… Data export in CSV and Excel formats

---

## ğŸ“¦ Requirements

- Python 3.9+
- Dependencies listed in `requirements.txt` (Playwright, pandas, openpyxl, etc.)

```bash
pip install -r requirements.txt
playwright install
```

---

## ğŸš€ Running the Pipeline

Run each step in order:

```bash
python scrapers/1_parse_sitemap.py
python scrapers/2_extract_facility_links.py
python scrapers/3_scrape_facility_details.py
python scrapers/4_enrich_with_google.py
```

You can re-run failed scrapes by processing the `failed_urls.csv`.

---

## ğŸ“Š Output

All output files are located in the `/data` directory:

| File                          | Description                                  |
|------------------------------|----------------------------------------------|
| `facility_urls.csv`          | URLs of city-specific facility listings      |
| `community_links.csv`        | URLs of individual facility pages            |
| `facility_data.xlsx`         | Raw scraped data (name, address, type)       |
| `facility_data_enriched.xlsx`| Enriched data (phone, website URL included)  |
| `failed_urls.csv`            | Failed requests for reprocessing             |

---

## ğŸ§‘â€ğŸ’» Author

**Martin Votava**  
GitHub: [martin-vot](https://github.com/martin-vot)  
Project ready for freelance or production use.

---

## ğŸ“ License

This project is licensed under the MIT License.
